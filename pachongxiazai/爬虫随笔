UA生成
pip3 install fake-useragent
from fake_useragent import UserAgent
ua = UserAgent()
#ie浏览器的user agent
print(ua.ie)
#opera浏览器
print(ua.opera)
#chrome浏览器
print(ua.chrome)
#firefox浏览器
print(ua.firefox)
#safri浏览器
print(ua.safari)
#随机UA
print(ua.random)

#直接请求
urllib
response = urllib.request.urlopen('http://python.org/')
html = response.read()
#requset请求
req = urllib.request.Request('http://python.org/')
response = urllib.request.urlopen(req)
the_page = response.read()

#附加数据
url = 'http://localhost/login.php'
user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
values = {
'act' : 'login',
'login[email]' : 'yzhang@i9i8.com',
'login[password]' : '123456'
}
data = urllib.parse.urlencode(values)
req = urllib.request.Request(url, data)
req.add_header('Referer', 'http://www.python.org/')
response = urllib.request.urlopen(req)
the_page = response.read()

#头部伪造
url = 'http://localhost/login.php'
user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
values = {
'act' : 'login',
'login[email]' : 'yzhang@i9i8.com',
'login[password]' : '123456'
}
headers = { 'User-Agent' : user_agent }
data = urllib.parse.urlencode(values)
req = urllib.request.Request(url, data, headers)
response = urllib.request.urlopen(req)
the_page = response.read()

#错误处理
try:
	urllib.request.urlopen(req)
except urllib.error.HTTPError as e:
	print(e.code)
	print(e.read().decode("utf8"))


#异常处理方式1
try:
	response = urlopen(req)
except HTTPError as e:
	print('The server couldn't fulfill the request.')
	print('Error code: ', e.code)
except URLError as e:
	print('We failed to reach a server.')
	print('Reason: ', e.reason)
else:
	print("good!")
	print(response.read().decode("utf8"))

#异常处理方式2
try:
	response = urlopen(req)
except URLError as e:
	if hasattr(e, 'reason'):
		print('We failed to reach a server.')
		print('Reason: ', e.reason)
	elif hasattr(e, 'code'):
		print('The server couldn't fulfill the request.')
		print('Error code: ', e.code)
else:
	print("good!")
	print(response.read().decode("utf8"))

#HTTP 认证
import urllib.request
# 创建密码管理器
password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm()
# 添加用户及密码
# 如果在这个域中则使用其
top_level_url = "https://www.111cn.net /"
password_mgr.add_password(None, top_level_url, 'rekfan', 'xxxxxx')
handler = urllib.request.HTTPBasicAuthHandler(password_mgr)
# 创建opener
opener = urllib.request.build_opener(handler)
# 使用opener请求
a_url = "https://www.111cn.net /"
x = opener.open(a_url)
print(x.read())
# opnner安装加载.
# 使用openner请求.
urllib.request.install_opener(opener)
a = urllib.request.urlopen(a_url).read().decode('utf8')
print(a)

#使用代理
proxy_support = urllib.request.ProxyHandler({'sock5': 'localhost:1080'})
opener = urllib.request.build_opener(proxy_support)
urllib.request.install_opener(opener)
a = urllib.request.urlopen("http://www.111cn.net ").read().decode("utf8")
print(a)

#超时设置
import socket
import urllib.request
# 超时秒数
timeout = 2
socket.setdefaulttimeout(timeout)
# 以上设置了超时
try:
	req = urllib.request.Request('http://www.111cn.net /')
except socket.timeout:
	pass
a = urllib.request.urlopen(req).read()
print(a)

#文件下载
urlretrieve(url, filename=None, reporthook=None, data=None)
参数 finename 指定了保存本地路径（如果参数未指定，urllib会生成一个临时文件保存数据。）
参数 reporthook 是一个回调函数，当连接上服务器、以及相应的数据块传输完毕时会触发该回调，我们可以利用这个回调函数来显示当前的下载进度。
参数 data 指 post 到服务器的数据。
该方法返回一个包含两个元素的(filename, headers)元组，filename 表示保存到本地的路径，header 表示服务器的响应头。
def cbk(a, b, c):
    '''回调函数
    @a: 已经下载的数据块
    @b: 数据块的大小
    @c: 远程文件的大小
    '''
    per = 100.0 * a * b / c
    if per > 100:
        per = 100
    print('%.2f%%' % per)
url = 'http://www.google.com'
local = 'd://google.html'
urllib.urlretrieve(url, local, cbk)

requests使用
import requests
r = requests.get('https://github.com/timeline.json')
r = requests.post("http://httpbin.org/post")
r = requests.put("http://httpbin.org/put")
r = requests.delete("http://httpbin.org/delete")
r = requests.head("http://httpbin.org/get")
r = requests.options("http://httpbin.org/get")

payload = {'key1': 'value1', 'key2': 'value2'}
r = requests.get("http://httpbin.org/get", params=payload)

payload = {'key1': 'value1', 'key2': ['value2', 'value3']}
r = requests.get('http://httpbin.org/get', params=payload)
r.text
r.encoding
r.encoding = 'ISO-8859-1'
r.content
r.json()
r.raw
r.status_code
r.status_code == requests.codes.ok
r.raise_for_status()
r.headers
r.cookies

headers = {'user-agent': 'my-app/0.0.1'}
r = requests.get(url, headers=headers)

payload = {'key1': 'value1', 'key2': 'value2'}
r = requests.post("http://httpbin.org/post", data=payload)

payload = (('key1', 'value1'), ('key1', 'value2'))
r = requests.post('http://httpbin.org/post', data=payload)

cookies = dict(cookies_are='working')
r = requests.get(url, cookies=cookies)
r.text

jar = requests.cookies.RequestsCookieJar()
jar.set('tasty_cookie', 'yum', domain='httpbin.org', path='/cookies')
jar.set('gross_cookie', 'blech', domain='httpbin.org', path='/elsewhere')
url = 'http://httpbin.org/cookies'
r = requests.get(url, cookies=jar)

默认情况下，除了 HEAD, Requests 会自动处理所有重定向。
可以使用响应对象的 history 方法来追踪重定向。
如果你使用的是GET、OPTIONS、POST、PUT、PATCH 或者 DELETE，那么你可以通过 allow_redirects 参数禁用重定向处理：
r = requests.get('http://github.com', allow_redirects=False)
如果你使用了 HEAD，你也可以启用重定向：
r = requests.head('http://github.com', allow_redirects=True)

requests 在经过以 timeout 参数设定的秒数时间之后停止等待响应。基本上所有的生产代码都应该使用这一参数

错误与异常
遇到网络问题（如：DNS 查询失败、拒绝连接等）时，Requests 会抛出一个 ConnectionError 异常。
如果 HTTP 请求返回了不成功的状态码， Response.raise_for_status() 会抛出一个 HTTPError 异常。
若请求超时，则抛出一个 Timeout 异常。
若请求超过了设定的最大重定向次数，则会抛出一个 TooManyRedirects 异常。
所有Requests显式抛出的异常都继承自 requests.exceptions.RequestException 。

会话对象让你能够跨请求保持某些参数。它也会在同一个 Session 实例发出的所有请求之间保持 cookie， 期间使用 urllib3 的 connection pooling 功能。所以如果你向同一主机发送多个请求，底层的 TCP 连接将会被重用，从而带来显著的性能提升。

s = requests.Session()
s.get('http://httpbin.org/cookies/set/sessioncookie/123456789')
r = s.get("http://httpbin.org/cookies")

可以通过传递一个 {hook_name: callback_function} 字典给 hooks请求参数为每个请求分配一个钩子函数
如果需要使用代理，可以通过为任意请求方法提供 proxies 参数来配置单个请求值为字典型，结构为 协议：地址，协议：地址


SELENIUM
0、下载 Selenium for python   https://pypi.python.org/pypi/selenium#downloads
python -m pip install selenium
1、基础篇，主要是能个熟悉如何打开和关闭浏览器，主要是三大浏览器IE/火狐/谷歌
1.1、要用selenium打开fiefox浏览器。首先需要去下载一个driver插件geckodriver.exe， 下载地址https://github.com/mozilla/geckodriver/releases，下载好这个exe文件后，把这个文件放到你的python安装目录下
from selenium import webdriver   # 导入webdriver包
driver = webdriver.Firefox()    # 初始化一个火狐浏览器实例：driver
driver = webdriver.Chrome()
driver = webdriver.Ie()  #注意ie目前稳定性不好不建议使用
driver.maximize_window()        # 最大化浏览器
driver.implicitly_wait(8) # 设置隐式时间等待
driver.get("https://www.baidu.com")  # 通过get()方法，打开一个url站点
driver.quit()     #关闭并退出浏览器

driver.get("https://www.baidu.com")  # 地址栏输入百度地址
driver.find_element_by_xpath("//*[@id='kw']").send_keys("selenium")  # 搜索输入框输入Selenium
driver.find_element_by_xpath("//*[@id='su']").click()  #点击百度一下按钮
# 导入time模块，等待2秒
time.sleep(2)
#这里通过元素XPath表达式来确定该元素显示在结果列表，从而判断Selenium官网这个链接显示在结果列表。
# 这里采用了相对元素定位方法/../
# 通过selenium方法is_displayed() 来判断我们的目标元素是否在页面显示。
driver.find_element_by_xpath("//div/h3/a[text()='官网']/../a/em[text()='Selenium']").is_displayed()

# 得到页面源代码
doc = driver.page_source
emails = re.findall(r'[\w]+@[\w\.-]+',doc) # 利用正则，找出 xxx@xxx.xxx 的字段，保存到emails列表

#利用id定位
driver.find_element_by_id("kw")
#利用tag_name定位
driver.find_element_by_tag_name("form")
#利用链接文字定位
driver.find_element_by_link_text("新闻")
#利用部分链接文字定位
driver.find_element_by_partial_link_text("主页").click()
#利用classname定位
driver.find_element_by_class_name("s_ipt")
#利用name定位
driver.find_element_by_name("wd")
#利用css选择器定位
driver.find_element_by_css_selector("#su")

send_keys()来输入字符串到文本输入框这样的页面元素
click()来点击页面上支持点击的元素
clear()清除一个文本输入框内的文字

driver.refresh() # 刷新方法 refresh
driver.back()  # 后退
driver.forward() # 前进
driver.capabilities['version']  # 返回浏览器version的值
driver.current_url #获取当前页面的url
driver.title  # title方法可以获取当前页面的标题显示的字段
ele = driver.find_element_by_tag_name('body').send_keys(Keys.CONTROL + 't')  # 触发ctrl + t
find_elements，也就是找一组元素，返回的是一个列表
driver.set_window_size(1280,800)  # 分辨率 1280*800
driver.get_window_size()	#获取窗口大小
element.text属性得到字符串
element.size属性获取元素的大小（宽高）
link.get_attribute('href')获取属性值
driver.get_screenshot_as_file("C:\\Users\\你的账户名\\Desktop\\baidu.png") #截图保存
is_selected()返回是是布尔值，用来判断单选或者多选控件是否被选中
driver.execute_script("window.alert('这是一个alert弹框。');") # 注意这里的分号是英文输入法的分号，不能用中文
driver.execute_script("return arguments[0].scrollIntoView();",target_elem) # 用目标元素参考去拖动
#driver.execute_script("scroll(0,2400)") # 这个是第二种方法，比较粗劣，大概的拖动






http://www.yiibai.com/selenium/selenium_ide.html
